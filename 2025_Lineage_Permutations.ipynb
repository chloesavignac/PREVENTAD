{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "755bbb15",
   "metadata": {},
   "source": [
    "# Loadings libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6ffe3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression, PLSCanonical\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "from nilearn.signal import clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ab43b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique PSCID_RBANS: 370\n"
     ]
    }
   ],
   "source": [
    "# Load participant-level data\n",
    "df = pd.read_csv('../prevent_AD_data_dummy_jan_2023_fam_hist.csv', index_col=0)\n",
    "\n",
    "# Load all time points (longitudinal visits)\n",
    "timepoints_df = pd.read_csv('../all_time_points_01.27.23.csv')\n",
    "\n",
    "# Extract PSCID list for participants with non-missing 50-month scores\n",
    "pscid_df = timepoints_df.dropna(subset=['50'])[['CandID', 'PSCID_RBANS']]\n",
    "pscid_list = pscid_df['PSCID_RBANS'].unique()\n",
    "print(f\"Number of unique PSCID_RBANS: {len(pscid_list)}\")\n",
    "\n",
    "# Merge relevant participant info with PSCID\n",
    "info_cols = ['only_mother', 'CandID', 'Sex_Female', 'APOE']\n",
    "infos = (\n",
    "    df[info_cols]\n",
    "    .drop_duplicates()\n",
    "    .merge(pscid_df, on='CandID', how='left')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3012a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load canonical variates from CCA at each time point\n",
    "cca_files = {\n",
    "    \"BL00\": '../BL00_CCA_modes.csv',\n",
    "    \"FU12\": '../FU12_CCA_modes.csv',\n",
    "    \"FU24\": '../FU24_CCA_modes.csv',\n",
    "    \"FU36\": '../FU36_CCA_modes.csv',\n",
    "    \"FU48\": '../FU48_CCA_modes.csv'\n",
    "}\n",
    "\n",
    "cca_dfs = [pd.read_csv(fname, index_col=1) for fname in cca_files.values()]\n",
    "all_cca = pd.concat(cca_dfs, keys=cca_files.keys()).drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eb7595",
   "metadata": {},
   "source": [
    "# Maternal vs Paternal "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8494ed42",
   "metadata": {},
   "source": [
    "## Importing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d55f4",
   "metadata": {},
   "source": [
    "### Hippocampus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f300f6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(912, 46)\n"
     ]
    }
   ],
   "source": [
    "# Load hippocampal (HC) data from various time points\n",
    "hc_files = {\n",
    "    \"BL00\": '../HC_segmentation/csv/BL00_HC_left_right.csv',\n",
    "    \"FU12\": '../HC_segmentation/csv/FU12_HC_left_right.csv',\n",
    "    \"FU24\": '../HC_segmentation/csv/FU24_HC_left_right.csv',\n",
    "    \"FU36\": '../HC_segmentation/csv/FU36_HC_left_right.csv',\n",
    "    \"FU48\": '../HC_segmentation/csv/FU48_HC_left_right.csv'\n",
    "}\n",
    "\n",
    "# Read and store HC data for each time point\n",
    "hc_dfs = [pd.read_csv(file, index_col=0) for file in hc_files.values()]\n",
    "\n",
    "# Combine all HC data into a single DataFrame with multi-level index (timepoint + participant)\n",
    "hc_volumes = pd.concat(hc_dfs, keys=hc_files.keys())\n",
    "\n",
    "# Ensure that the HC data aligns with the canonical variates index (all_cca)\n",
    "hc_volumes = hc_volumes.loc[all_cca.index]\n",
    "\n",
    "# Print the shape to verify data alignment\n",
    "print(hc_volumes.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a34e2031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index and rename columns for clarity\n",
    "hc_volumes = hc_volumes.reset_index().rename(columns={\n",
    "    'level_0': 'visit',  # Rename the first index level to 'visit'\n",
    "    'level_1': 'PSCID'         # Rename the second index level to 'PSCID'\n",
    "})\n",
    "\n",
    "# Merge the HC data with the 'infos' DataFrame based on 'PSCID'\n",
    "hc_volumes = hc_volumes.merge(infos, left_on='PSCID', right_on='PSCID_RBANS', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0041a9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "# Extract base list of hippocampal column names from BL00_HC, excluding the last 4\n",
    "col_HC = list(hc_volumes.iloc[:,2:-6].columns)\n",
    "\n",
    "# Define columns to remove explicitly for clarity and safety\n",
    "exclude_cols = [\n",
    "    'Whole_hippocampal_body_left',\n",
    "    'Whole_hippocampal_head_left',\n",
    "    'Whole_hippocampus_left',\n",
    "    'PSCID_left',\n",
    "    'Whole_hippocampal_body_right',\n",
    "    'Whole_hippocampal_head_right',\n",
    "    'Whole_hippocampus_right',\n",
    "    'PSCID_right',\n",
    "]\n",
    "\n",
    "# Remove columns only if they exist in the list\n",
    "col_HC = [col for col in col_HC if col not in exclude_cols]\n",
    "\n",
    "print(len(col_HC))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "084bf977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization and normalization\n",
    "for col in col_HC:\n",
    "    hc_volumes[col] = StandardScaler().fit_transform(hc_volumes[[col]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df48fc",
   "metadata": {},
   "source": [
    "### Default Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d40e117e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(912, 91)\n"
     ]
    }
   ],
   "source": [
    "# Load default network (DN) volume data from multiple time points\n",
    "dn_files = {\n",
    "    \"BL00\": '../BL00_DN.csv',\n",
    "    \"FU12\": '../FU12_DN.csv',\n",
    "    \"FU24\": '../FU24_DN.csv',\n",
    "    \"FU36\": '../FU36_DN.csv',\n",
    "    \"FU48\": '../FU48_DN.csv'\n",
    "}\n",
    "\n",
    "# Read and store DN data for each time point\n",
    "dn_dfs = [pd.read_csv(file, index_col=0) for file in dn_files.values()]\n",
    "\n",
    "# Combine all DN data into a single DataFrame with multi-level index (timepoint + participant)\n",
    "dn_volumes = pd.concat(dn_dfs, keys=dn_files.keys())\n",
    "\n",
    "# Ensure that the DN data aligns with the canonical variates index (all_cca)\n",
    "dn_volumes = dn_volumes.loc[all_cca.index]\n",
    "\n",
    "# Print the shape to verify data alignment\n",
    "print(dn_volumes.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01539abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names\n",
    "dn_volumes.columns = [col.split('Networks_')[-1] if i > 0 else col for i, col in enumerate(dn_volumes.columns)]\n",
    "\n",
    "# Store cleaned column names for reference\n",
    "dn_cols = dn_volumes.columns\n",
    "\n",
    "# Reset multi-index and rename to meaningful labels\n",
    "dn_volumes = dn_volumes.reset_index().rename(columns={'level_0': 'visit', 'level_1': 'PSCID'})\n",
    "\n",
    "# Merge with participant info using PSCID\n",
    "dn_volumes = dn_volumes.merge(infos, left_on='PSCID', right_on='PSCID_RBANS', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c469a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization and normalization\n",
    "for col in dn_cols:\n",
    "    dn_volumes[col] = StandardScaler().fit_transform(dn_volumes[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad687798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pval(coefs, real_coefs):\n",
    "    \"\"\"\n",
    "    Compute empirical one-sided p-values based on null distribution in `coefs`\n",
    "    compared to observed values in `real_coefs`.\n",
    "\n",
    "    Parameters:\n",
    "    - coefs (pd.DataFrame): Null distribution for each coefficient (shape: permutations × variables).\n",
    "    - real_coefs (list or np.ndarray): Observed coefficients (length must match number of columns in `coefs`).\n",
    "\n",
    "    Returns:\n",
    "    - dict: Variable name → p-value.\n",
    "    \"\"\"\n",
    "    pvals = {}\n",
    "    for col, obs_val in zip(coefs.columns, real_coefs):\n",
    "        null_dist = coefs[col]\n",
    "\n",
    "        if obs_val < 0:\n",
    "            pval = (null_dist < obs_val).sum() / len(null_dist)\n",
    "        else:\n",
    "            pval = (null_dist > obs_val).sum() / len(null_dist)\n",
    "\n",
    "        pvals[col] = pval\n",
    "\n",
    "    return pvals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f26438f",
   "metadata": {},
   "source": [
    "## Permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f869d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '25.05.13'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce7fc73",
   "metadata": {},
   "source": [
    "### Females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d440e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Permutations for female model: 100%|██████████| 1000/1000 [06:26<00:00,  2.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# Function to fit null models in parallel\n",
    "def fit_null_model(X_scaled, y, seed):\n",
    "    shuffled_y = np.random.default_rng(seed).permutation(y)\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(X_scaled, shuffled_y)\n",
    "    return model.coef_[0]\n",
    "\n",
    "# Storage for results\n",
    "all_coefs_f = pd.DataFrame(columns=col_HC)\n",
    "pval_dfs = []\n",
    "\n",
    "# Begin permutation loop\n",
    "for seed in tqdm(range(1000), desc=\"Permutations for female model\"):\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Stratified sampling\n",
    "    group_indices = {\n",
    "        'females_mat': np.where((hc_volumes.only_mother == 1) & (hc_volumes.Sex_Female == 1))[0],\n",
    "        'females_pat': np.where((hc_volumes.only_mother == 0) & (hc_volumes.Sex_Female == 1))[0],\n",
    "    }\n",
    "\n",
    "    sampled_indices = np.concatenate([\n",
    "        random.sample(list(group_indices['females_mat']), 50),\n",
    "        random.sample(list(group_indices['females_pat']), 50),\n",
    "    ])\n",
    "\n",
    "    subset = hc_volumes.iloc[sampled_indices].sort_index()\n",
    "    X_females = np.array(subset[col_HC])\n",
    "    y_females = subset.only_mother.values\n",
    "\n",
    "    # Fit real model\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(X_females, y_females)\n",
    "    real_coefs = model.coef_[0]\n",
    "    real_coefs_df = pd.DataFrame(model.coef_, columns=col_HC)\n",
    "    all_coefs_f = pd.concat([all_coefs_f, real_coefs_df], ignore_index=True)\n",
    "    \n",
    "    # Build null distribution in parallel\n",
    "    null_seeds = range(0,1000)\n",
    "    null_coefs = Parallel(n_jobs=6)(delayed(fit_null_model)(X_females, y_females, s) for s in null_seeds)\n",
    "    null_coefs_df = pd.DataFrame(null_coefs, columns=col_HC)\n",
    "\n",
    "    # Compute p-values\n",
    "    dict_pval = compute_pval(null_coefs_df, real_coefs)\n",
    "    pval_df = pd.DataFrame({\n",
    "        'coefs': list(dict_pval.keys()),\n",
    "        f'pval_{seed}': list(dict_pval.values())\n",
    "    })\n",
    "    pval_dfs.append(pval_df)\n",
    "\n",
    "# Save all coefficients\n",
    "all_coefs_f.to_csv(f'{date}/brain_imaging/diagnostic_test/all_coefs_females_prevent_AD_hc_2025.csv', index=False)\n",
    "\n",
    "# Combine p-values after loop\n",
    "all_pvals_f = reduce(lambda left, right: left.merge(right, on='coefs'), pval_dfs)\n",
    "# Save results\n",
    "all_pvals_f.to_csv(f'{date}/brain_imaging/diagnostic_test/prevent_ad_hc_all_pvals_f_1000_2025.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4713bd",
   "metadata": {},
   "source": [
    "### Males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1158411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Permutations for male model: 100%|██████████| 1000/1000 [08:04<00:00,  2.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# Function to fit null model with shuffled outcome\n",
    "def fit_null_model(X_scaled, y, seed):\n",
    "    shuffled_y = np.random.default_rng(seed).permutation(y)\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(X_scaled, shuffled_y)\n",
    "    return model.coef_[0]\n",
    "\n",
    "# Store results across permutations\n",
    "all_coefs_m = pd.DataFrame(columns=col_HC)\n",
    "pval_dfs_m = []\n",
    "\n",
    "for seed in tqdm(range(1000), desc=\"Permutations for male model\"):\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Stratified sampling\n",
    "    group_indices = {\n",
    "        'males_mat':   np.where((hc_volumes.only_mother == 1) & (hc_volumes.Sex_Female == 0))[0],\n",
    "        'males_pat':   np.where((hc_volumes.only_mother == 0) & (hc_volumes.Sex_Female == 0))[0],\n",
    "    }\n",
    "\n",
    "    sampled_indices = np.concatenate([\n",
    "        random.sample(list(group_indices['males_mat']), 50),\n",
    "        random.sample(list(group_indices['males_pat']), 50),\n",
    "    ])\n",
    "\n",
    "    subset = hc_volumes.iloc[sampled_indices].sort_index()\n",
    "    X_males = np.array(subset[col_HC])\n",
    "    y_males = subset.only_mother.values\n",
    "\n",
    "    # Fit real model\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(X_males, y_males)\n",
    "    real_coefs = model.coef_[0]\n",
    "    real_coefs_df = pd.DataFrame(model.coef_, columns=col_HC)\n",
    "    all_coefs_m = pd.concat([all_coefs_m, real_coefs_df], ignore_index=True)\n",
    "\n",
    "    # Build null distribution in parallel\n",
    "    null_seeds = range(0,1000)\n",
    "    null_coefs = Parallel(n_jobs=6)(\n",
    "        delayed(fit_null_model)(X_males, y_males, s) for s in null_seeds\n",
    "    )\n",
    "    null_coefs_df = pd.DataFrame(null_coefs, columns=col_HC)\n",
    "\n",
    "    # Compute p-values\n",
    "    dict_pval = compute_pval(null_coefs_df, real_coefs)\n",
    "    pval_df = pd.DataFrame({\n",
    "        'coefs': list(dict_pval.keys()),\n",
    "        f'pval_{seed}': list(dict_pval.values())\n",
    "    })\n",
    "    pval_dfs_m.append(pval_df)\n",
    "\n",
    "# Save all coefficients\n",
    "all_coefs_m.to_csv(f'{date}/brain_imaging/diagnostic_test/all_coefs_males_prevent_AD_hc_2025.csv', index=False)\n",
    "\n",
    "# Combine p-values after loop\n",
    "all_pvals_m = reduce(lambda left, right: left.merge(right, on='coefs'), pval_dfs_m)\n",
    "# Save results\n",
    "all_pvals_m.to_csv(f'{date}/brain_imaging/diagnostic_test/prevent_ad_hc_all_pvals_m_1000_2025.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ba3f66",
   "metadata": {},
   "source": [
    "## Default Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e1103e",
   "metadata": {},
   "source": [
    "### Males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60889791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Permutations for DN male model:  36%|███▌      | 362/1000 [08:47<23:00,  2.16s/it]   /Users/chloesavignac/.conda/envs/masters_projects/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "Permutations for DN male model: 100%|██████████| 1000/1000 [28:13<00:00,  1.69s/it]   \n"
     ]
    }
   ],
   "source": [
    "# Function to fit null model with shuffled outcome\n",
    "def fit_null_model(X_scaled, y, seed):\n",
    "    shuffled_y = np.random.default_rng(seed).permutation(y)\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(X_scaled, shuffled_y)\n",
    "    return model.coef_[0]\n",
    "\n",
    "# Store results across permutations\n",
    "all_coefs_m = pd.DataFrame(columns=dn_cols)\n",
    "pval_dfs_m = []\n",
    "\n",
    "for seed in tqdm(range(1000), desc=\"Permutations for DN male model\"):\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Stratified sampling\n",
    "    group_indices = {\n",
    "        'males_mat':   np.where((dn_volumes.only_mother == 1) & (dn_volumes.Sex_Female == 0))[0],\n",
    "        'males_pat':   np.where((dn_volumes.only_mother == 0) & (dn_volumes.Sex_Female == 0))[0],\n",
    "    }\n",
    "\n",
    "    sampled_indices = np.concatenate([\n",
    "        random.sample(list(group_indices['males_mat']), 50),\n",
    "        random.sample(list(group_indices['males_pat']), 50),\n",
    "    ])\n",
    "\n",
    "    subset = dn_volumes.iloc[sampled_indices].sort_index()\n",
    "    X_males = np.array(subset[dn_cols])\n",
    "    y_males = subset.only_mother.values\n",
    "\n",
    "    # Fit real model\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(X_males, y_males)\n",
    "    real_coefs = model.coef_[0]\n",
    "    real_coefs_df = pd.DataFrame(model.coef_, columns=dn_cols)\n",
    "    all_coefs_m = pd.concat([all_coefs_m, real_coefs_df], ignore_index=True)\n",
    "\n",
    "    # Build null distribution in parallel\n",
    "    null_seeds = range(0,1000)\n",
    "    null_coefs = Parallel(n_jobs=6)(\n",
    "        delayed(fit_null_model)(X_males, y_males, s) for s in null_seeds\n",
    "    )\n",
    "    null_coefs_df = pd.DataFrame(null_coefs, columns=dn_cols)\n",
    "\n",
    "    # Compute p-values\n",
    "    dict_pval = compute_pval(null_coefs_df, real_coefs)\n",
    "    pval_df = pd.DataFrame({\n",
    "        'coefs': list(dict_pval.keys()),\n",
    "        f'pval_{seed}': list(dict_pval.values())\n",
    "    })\n",
    "    pval_dfs_m.append(pval_df)\n",
    "\n",
    "# Save results\n",
    "all_coefs_m.to_csv(f'{date}/brain_imaging/diagnostic_test/all_coefs_males_prevent_AD_dn_2025.csv', index=False)\n",
    "\n",
    "# Combine p-values after loop\n",
    "all_pvals_m = reduce(lambda left, right: left.merge(right, on='coefs'), pval_dfs_m)\n",
    "\n",
    "# Save results\n",
    "all_pvals_m.to_csv(f'{date}/brain_imaging/diagnostic_test/prevent_ad_dn_all_pvals_m_1000_2025.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780cb95b",
   "metadata": {},
   "source": [
    "### Females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "203f16b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Permutations for DN female model: 100%|██████████| 1000/1000 [10:05<00:00,  1.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Function to fit null model with shuffled outcome\n",
    "def fit_null_model(X_scaled, y, seed):\n",
    "    shuffled_y = np.random.default_rng(seed).permutation(y)\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(X_scaled, shuffled_y)\n",
    "    return model.coef_[0]\n",
    "\n",
    "# Store results across permutations\n",
    "all_coefs_f = pd.DataFrame(columns=dn_cols)\n",
    "pval_dfs_f = []\n",
    "\n",
    "for seed in tqdm(range(1000), desc=\"Permutations for DN female model\"):\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Stratified sampling\n",
    "    group_indices = {\n",
    "        'females_mat': np.where((dn_volumes.only_mother == 1) & (dn_volumes.Sex_Female == 1))[0],\n",
    "        'females_pat': np.where((dn_volumes.only_mother == 0) & (dn_volumes.Sex_Female == 1))[0],\n",
    "    }\n",
    "\n",
    "    sampled_indices = np.concatenate([\n",
    "        random.sample(list(group_indices['females_mat']), 50),\n",
    "        random.sample(list(group_indices['females_pat']), 50),\n",
    "    ])\n",
    "\n",
    "    subset = dn_volumes.iloc[sampled_indices].sort_index()\n",
    "    X_females = np.array(subset[dn_cols])\n",
    "    y_females = subset.only_mother.values\n",
    "\n",
    "    # Fit real model\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(X_females, y_females)\n",
    "    real_coefs = model.coef_[0]\n",
    "    real_coefs_df = pd.DataFrame(model.coef_, columns=dn_cols)\n",
    "    all_coefs_f = pd.concat([all_coefs_f, real_coefs_df], ignore_index=True)\n",
    "\n",
    "    # Build null distribution in parallel\n",
    "    null_seeds = range(0,1000)\n",
    "    null_coefs = Parallel(n_jobs=6)(\n",
    "        delayed(fit_null_model)(X_females, y_females, s) for s in null_seeds\n",
    "    )\n",
    "    null_coefs_df = pd.DataFrame(null_coefs, columns=dn_cols)\n",
    "\n",
    "    # Compute p-values\n",
    "    dict_pval = compute_pval(null_coefs_df, real_coefs)\n",
    "    pval_df = pd.DataFrame({\n",
    "        'coefs': list(dict_pval.keys()),\n",
    "        f'pval_{seed}': list(dict_pval.values())\n",
    "    })\n",
    "    pval_dfs_f.append(pval_df)\n",
    "\n",
    "# Save results\n",
    "all_coefs_f.to_csv(f'{date}/brain_imaging/diagnostic_test/all_coefs_females_prevent_AD_dn_2025.csv', index=False)\n",
    "\n",
    "# Combine p-values after loop\n",
    "all_pvals_f = reduce(lambda left, right: left.merge(right, on='coefs'), pval_dfs_f)\n",
    "# Save results\n",
    "all_pvals_f.to_csv(f'{date}/brain_imaging/diagnostic_test/prevent_ad_dn_all_pvals_f_1000_2025.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
